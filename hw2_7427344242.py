# -*- coding: utf-8 -*-
"""hw2_7427344242.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uNmjsNBVCCHKs_zVHoj4GvR7U5o1lgk2
"""

#python 3.10.12

import numpy as np
import json

"""#Read train.json and extract sentences from the *json*"""

train_file_path = 'data/train.json'
dev_file_path = 'data/dev.json'
test_file_path = 'data/test.json'

with open(train_file_path) as f:
  train_data_ini = json.load(f)

"""#Find frequencies of each word and then choose a threshold to replace the value of word with freq < threshold with '< unk >'"""

# Extract words and labels from the sentence index in the training data. The sentences are already tokenized
words = []
train_labels = []
for i in train_data_ini:
    words.extend(i['sentence'])
    for elem in i['labels']:
      if elem not in train_labels:
        train_labels.append(elem)

word_freq = dict()
for word in words:
  if word not in word_freq.keys():
    word_freq[word] = 1
  else:
    word_freq[word] += 1

"""#Threshold = 3"""

#random threshold of 2
threshold = 2

# Initialize the vocabulary with the special token
vocab = {"< unk >": 0}
unk_freq = 0

# Replace rare words and create the vocabulary
for word, freq in word_freq.items():
    if freq < threshold:
        word = "< unk >"
        unk_freq += freq
    else:
      vocab[word] = freq

vocab['< unk >'] = unk_freq

# Separate the first key-value pair
first_key, first_value = next(iter(vocab.items()))
rest_of_vocab = dict(list(vocab.items())[1:])

# Sort the remaining key-value pairs by values in decreasing order
sorted_items = sorted(rest_of_vocab.items(), key=lambda item: item[1], reverse=True)

# Combine the first key-value pair and the sorted key-value pairs
sorted_vocab = {first_key: first_value, **dict(sorted_items)}

def deep_copy_dict(d):
    if not isinstance(d, dict):
        return d  # If it's not a dictionary, return it as is (base case)

    new_dict = {}
    for key, value in d.items():
        new_key = deep_copy_dict(key)  # Recursively copy keys
        new_value = deep_copy_dict(value)  # Recursively copy values
        new_dict[new_key] = new_value

    return new_dict

def process_dataset_copy(dataset, vocab):
    # Create a copy of the dataset without modifying the original data
    processed_dataset = []

    for sentence_data in dataset:
        # Create a new dictionary for each sentence_data
        new_sentence_data = deep_copy_dict(sentence_data)

        # Create a copy of the 'sentence' list to avoid modifying the original list
        sentence = sentence_data['sentence'][:]

        for i in range(len(sentence)):
            word = sentence[i]

            if word not in vocab.keys():
                sentence[i] = "< unk >"

        # Update the 'sentence' key in the new_sentence_data dictionary
        new_sentence_data['sentence'] = sentence

        processed_dataset.append(new_sentence_data)

    return processed_dataset

train_data = process_dataset_copy(train_data_ini, sorted_vocab)

"""#vocab.txt"""

# Save the vocabulary to vocab.txt
index = 0
with open('out/vocab.txt', 'w') as vocab_file:

    # Write the rest of the vocabulary in descending order of frequency
    for word, freq in sorted_vocab.items():
      vocab_file.write(f"{word}\t{index}\t{sorted_vocab[word]}\n")
      index += 1

'''print("Threshold value: ", threshold)
print("Vocab size: ", len(sorted_vocab))
print("Frequency of < unk >: ", sorted_vocab['< unk >'])'''

"""#Calculating initial state, transition and emission probabilities


"""

# Initialize dictionaries to store counts
# Initialize counters
transition_counts = {}  # count(s -> s')
emission_counts = {}    # count(s -> x)
initial_state_counts = {}  # count(null -> s)
overall_state_counts = {}
total_no_of_sentences = len(train_data)

# Iterate through training data to populate counters
for sentence_data in train_data:
    sentence = sentence_data['sentence']
    labels = sentence_data['labels']

    # Initialize the initial state (null -> s)
    initial_state_counts[labels[0]] = initial_state_counts.get(labels[0], 0) + 1 #checks if the label is already in the ditionary and if yes, retrives that value and incremnts or else initlizaes to zero and increments

    # Iterate through sentence pairs (s, s') and (s, x)
    for i in range(len(sentence)):
        s = labels[i]
        if i < len(labels)-1:
          next_s = labels[i + 1]
        else:
          next_s = None

        # Count transitions (s -> s')
        if next_s:
          t_key = (s, next_s)
          transition_counts[t_key] = transition_counts.get(t_key, 0) + 1

        #Count emissions (s -> x )
        word = sentence[i]
        e_key = (s, word)
        emission_counts[e_key] = emission_counts.get(e_key, 0) + 1

        #count each state frequency
        overall_state_counts[s] = overall_state_counts.get(s,0) + 1

"""#Calculate probabilitites"""

transition_probabilities = {}  # Dictionary to store transition probabilities

# Calculate transition probabilities
for (s, next_s), count in transition_counts.items():
    total_t_s = overall_state_counts.get(s, 0)
    transition_probabilities[(s, next_s)] = count / total_t_s

initial_probabilities = {}

for s, count in initial_state_counts.items():
  initial_probabilities[s] = count/total_no_of_sentences

emission_probabilities = {}  # Dictionary to store transition probabilities

# Calculate emission probabilities
for (s, word), count in emission_counts.items():
    total_e_s = overall_state_counts.get(s, 0)

    # Calculate the probability t(s' | s)
    emission_probabilities[(s, word)] = count / total_e_s

"""#Save as hmm.json"""

# Create the dictionaries for transition and emission parameters
initial = {f"{str(s)}": i_prob for s, i_prob in initial_probabilities.items()}
transition = {f"({str(s)},{str(next_s)})": t_prob for (s, next_s), t_prob in transition_probabilities.items()}
emission = {f"({str(s)},{word})": e_prob for (s, word), e_prob in emission_probabilities.items()}

# Create the overall HMM model dictionary
hmm_model = {
    'initial' : initial,
    'transition': transition,
    'emission': emission
}

# Specify the file path where you want to save the HMM model
model_file_path = 'out/hmm.json'

# Save the HMM model to a JSON file
with open(model_file_path, 'w') as f1:
    json.dump(hmm_model, f1, indent=4)

#print(f'HMM model saved to {model_file_path}')

"""#Greedy Decoding"""

def greedy_decoding(sentence, i_prob, t_prob, e_prob):
  num_words = len(sentence)
  predicted_tags = []
  most_probable_s = max(i_prob, key=lambda s: i_prob[s])

  for i in range(num_words):
    if i == 0:
        final_prob_of_s = float('-inf')
        for s in train_labels:
          ini_prob_of_s = i_prob.get(s,1e-6) * e_prob.get((s,sentence[0]), 1e-6)
          if ini_prob_of_s > final_prob_of_s :
            final_prob_of_s = ini_prob_of_s
            most_probable_s = s
    else:
        # Calculate the tag with the highest probability given the previous tag
        prev_tag = predicted_tags[i - 1]
        final_prob_of_s = float('-inf')
        for s in train_labels:
          prob_of_s = t_prob.get((prev_tag,s), 1e-6) * e_prob.get((s,sentence[i]), 1e-6)
          if prob_of_s > final_prob_of_s :
            final_prob_of_s = prob_of_s
            most_probable_s = s

    predicted_tags.append(most_probable_s)

  return predicted_tags

def accuracy_greedy(data, i_prob, t_prob, e_prob):
  total_no_of_labels = 0
  correct = 0
  for i in range(len(data)):
    sentence = data[i]['sentence']
    actual_labels = data[i]['labels']
    total_no_of_labels += len(actual_labels)
    pred_labels = greedy_decoding(sentence, i_prob, t_prob, e_prob)
    for j in range(len(actual_labels)):
      if pred_labels[j] == actual_labels[j]:
        correct += 1
  return (correct/total_no_of_labels)

"""#Test on dev data"""

with open(dev_file_path) as f:
  dev_data_ini = json.load(f)

dev_data = process_dataset_copy(dev_data_ini,sorted_vocab)

#print(accuracy_greedy(dev_data, initial_probabilities, transition_probabilities, emission_probabilities))

"""#Use test data and store results in hmm.json"""

with open(test_file_path) as f:
  test_data_ini = json.load(f)

test_data = process_dataset_copy(test_data_ini, sorted_vocab)

def greedy_on_test_data(orig_test_data, test_data, i_prob, t_prob, e_prob):
    predictions = []

    for orig_sentence_data, sentence_data in zip(orig_test_data, test_data):
        sentence = sentence_data['sentence']

        # Predict part-of-speech tags using greedy decoding
        pred_labels = greedy_decoding(sentence, i_prob, t_prob, e_prob)

        pred_dict = {
            'index': orig_sentence_data['index'],  # Use the index from orig_test_data
            'sentence': orig_sentence_data['sentence'],  # Use the sentence from orig_test_data
            'labels': pred_labels
        }

        predictions.append(pred_dict)

    return predictions

test_predictions = greedy_on_test_data(test_data_ini, test_data, initial_probabilities, transition_probabilities, emission_probabilities)

# Specify the file path
greedy_file_path = 'out/greedy.json'

# Save the predictions to a JSON file
with open(greedy_file_path, 'w') as f2:
    json.dump(test_predictions, f2, indent = 4)

"""#Viterbi Decoding"""

def viterbi_decoding(sentence, i_prob, t_prob, e_prob, labels):
  O = sentence
  S = labels #['NNP','VB'...]
  trellis = np.zeros((len(S), len(O))) #stores probs
  pointers = np.zeros((len(S), len(O)))  #stores indices

  for s in range(len(S)):
    trellis[s,0] = i_prob.get(S[s], 1e-6) * e_prob.get((S[s],sentence[0]), 1e-6)

  for o in range(1, len(O)):
    for s in range(len(S)):
        max_prob = float('-inf')
        max_prev_state_index = -1
        for k in range(len(S)):
            prob_for_state_at_k = trellis[k, o - 1] * t_prob.get((S[k], S[s]),1e-6) * e_prob.get((S[s],O[o]),1e-6)
            if prob_for_state_at_k > max_prob:
                max_prob = prob_for_state_at_k
                max_prev_state_index = k
        trellis[s, o] = max_prob
        pointers[s, o] = max_prev_state_index #index is stored of the state

  best_path = list()
  best_final_state_index = np.argmax(trellis[:, -1])
  best_path.append(best_final_state_index)

  for o in reversed(range(1, len(O))):
      prev_state_index = int(pointers[best_path[-1], o])
      best_path.append(prev_state_index)

  best_path.reverse()  # Reverse to get the path in the correct order

  best_path = [S[state_index] for state_index in best_path]

  return best_path

def accuracy_viterbi(data, i_prob, t_prob, e_prob, labels):
  total_no_of_labels = 0
  correct = 0
  for i in range(len(data)):
    sentence = data[i]['sentence']
    actual_labels = data[i]['labels']
    total_no_of_labels += len(actual_labels)
    pred_labels = viterbi_decoding(sentence, i_prob, t_prob, e_prob, labels)
    for j in range(len(actual_labels)):
      if pred_labels[j] == actual_labels[j]:
        correct += 1
  return (correct/total_no_of_labels)

#accuracy_viterbi(dev_data,initial_probabilities,transition_probabilities,emission_probabilities, train_labels)

def viterbi_on_test_data(orig_test_data,test_data, i_prob, t_prob, e_prob, labels):
    predictions = []

    for orig_sentence_data, sentence_data in zip(orig_test_data, test_data):
        sentence = sentence_data['sentence']

        # Predict part-of-speech tags using greedy decoding
        pred_labels = viterbi_decoding(sentence, i_prob, t_prob, e_prob, labels)

        pred_dict = {
            'index': orig_sentence_data['index'],  # Use the index from orig_test_data
            'sentence': orig_sentence_data['sentence'],  # Use the sentence from orig_test_data
            'labels': pred_labels
        }

        predictions.append(pred_dict)

    return predictions

test_predictions = viterbi_on_test_data(test_data_ini, test_data, initial_probabilities, transition_probabilities, emission_probabilities, train_labels)

# Specify the file path
viterbi_file_path = 'out/viterbi.json'

# Save the predictions to a JSON file
with open(viterbi_file_path, 'w') as f3:
    json.dump(test_predictions, f3, indent = 4)